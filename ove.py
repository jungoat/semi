import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from diffusers import UNet2DModel
from tqdm import tqdm

from dataset import DefectPatchDataset

# --- ì„¤ì • (03_train_flowmatch.pyì™€ ë™ì¼í•˜ê²Œ) ---
image_size = 64
num_classes = 12
cfg_drop_prob = 0.1
device = "cuda"
batch_size = 64 # ì‹¤ì œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
learning_rate = 5e-4 # ì•ˆì •ì ì¸ í•™ìŠµë¥ ë¡œ í…ŒìŠ¤íŠ¸

# --- ë°ì´í„° ì¤€ë¹„ (ë”± í•œ ë°°ì¹˜ë§Œ!) ---
csv_path = 'data/processed/labeled_defects.csv'
patches_path = 'data/processed/patches/'
dataset = DefectPatchDataset(csv_file=csv_path, img_dir=patches_path, image_size=image_size)
# num_workers=0 ìœ¼ë¡œ ì„¤ì •í•´ì•¼ ë””ë²„ê¹…ì´ í¸í•©ë‹ˆë‹¤.
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0) 

# ë°ì´í„°ë¡œë”ì—ì„œ ë”± í•œ ë°°ì¹˜ë§Œ êº¼ë‚´ì˜µë‹ˆë‹¤.
try:
    one_batch = next(iter(dataloader))
    print(f"Testing with one batch of size: {one_batch[0].shape}")
except StopIteration:
    print("Error: DataLoader is empty. Check your dataset.")
    exit()

# --- ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì¤€ë¹„ ---
model = UNet2DModel(
    sample_size=image_size, in_channels=1, out_channels=1, layers_per_block=2,
    block_out_channels=(32, 64, 128, 128),
    down_block_types=("DownBlock2D", "DownBlock2D", "AttnDownBlock2D", "DownBlock2D"),
    up_block_types=("UpBlock2D", "AttnUpBlock2D", "UpBlock2D", "UpBlock2D"),
    num_class_embeds=num_classes + 1,
).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# --- ë‹¨ì¼ ë°°ì¹˜ ê³¼ì í•© í…ŒìŠ¤íŠ¸ ë£¨í”„ ---
print("ğŸš€ Starting single batch overfitting test...")
num_iterations = 500 # ì´ ë°°ì¹˜ë¥¼ 500ë²ˆ ë°˜ë³µí•´ì„œ ë³´ì—¬ì¤Œ
model.train()

for i in tqdm(range(num_iterations)):
    optimizer.zero_grad()
    
    x1, c = one_batch
    x1 = x1.to(device)
    c = c.to(device)
    
    # Flow Matching ë¡œì§ (ë™ì¼)
    x0 = torch.randn_like(x1)
    t = torch.rand(x1.size(0), device=device).view(-1, 1, 1, 1)
    xt = (1 - t) * x0 + t * x1
    ut = x1 - x0
    
    context_mask = torch.rand(x1.size(0), device=device) > cfg_drop_prob
    c = torch.where(context_mask, c, num_classes)
    
    predicted_velocity = model(sample=xt, timestep=t.squeeze(), class_labels=c).sample
    loss = F.l1_loss(predicted_velocity, ut)
    
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # í´ë¦¬í•‘ ìœ ì§€
    optimizer.step()
    
    if (i + 1) % 50 == 0:
        print(f"Iteration {i+1}/{num_iterations}, Loss: {loss.item():.6f}")

print("âœ… Overfitting test finished!")